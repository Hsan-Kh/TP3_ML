# TP 3 : Apprentissage Non SupervisÃ©

## ğŸ“‹ Description

Ce TP explore les techniques de **clustering non supervisÃ©** appliquÃ©es Ã  la reconnaissance de chiffres manuscrits. Deux algorithmes principaux sont Ã©tudiÃ©s :
- **Classification Ascendante HiÃ©rarchique (CAH)**
- **K-Moyennes (K-means)**

## ğŸ¯ Objectifs

- DÃ©velopper et Ã©valuer des modÃ¨les de classification non supervisÃ©e
- Comparer les performances de la CAH et du K-means
- Analyser la structure hiÃ©rarchique des donnÃ©es via des dendrogrammes
- Identifier des groupements naturels dans les chiffres manuscrits

## ğŸ“Š Jeu de donnÃ©es : Digits

Le dataset **Digits** de scikit-learn contient :
- **1797 images** de chiffres manuscrits (0-9)
- Images en **niveaux de gris 8Ã—8 pixels** (64 features par image)
- Valeurs de pixels variant de **0 Ã  16**

## ğŸ› ï¸ Technologies utilisÃ©es

```python
- Python 3.x
- scikit-learn
- scipy
- numpy
- matplotlib
- seaborn
```

## ğŸ“ Structure du TP

### Partie 1 : Exploration des donnÃ©es
1. Importation du jeu de donnÃ©es Digits
2. DÃ©termination des dimensions
3. Affichage de la description
4. Visualisation d'Ã©chantillons d'images

### Partie 2 : Classification Ascendante HiÃ©rarchique (CAH)
5. GÃ©nÃ©ration de la matrice des liens avec `linkage()`
6. Visualisation du dendrogramme
7. DÃ©coupage du dendrogramme avec `fcluster()`
8. Attribution des classes aux images
9. InterprÃ©tation des groupements formÃ©s

### Partie 3 : K-Moyennes (K-means)
10. Clustering avec `KMeans` (10 clusters)
11. Visualisation des rÃ©sultats

### Partie 4 : Comparaison des mÃ©thodes
12. Tableau de contingence (confusion matrix)
13. Analyse comparative CAH vs K-means

## ğŸ“ Structure du projet

```
TP3_ML/
â”‚
â”œâ”€â”€ Unsupervised_Learning.ipynb        # Notebook Jupyter 
â””â”€â”€ requirements.txt            # DÃ©pendances du projet
```

## ğŸš€ Installation

### 1. CrÃ©er un environnement virtuel (recommandÃ©)
```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

### 2. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

## ğŸ’» Utilisation

### Lancer le notebook Jupyter
```bash
jupyter notebook Unsupervised_Learning.ipynb
```

### Exemple de code rapide
```python
# Charger le dataset
from sklearn import datasets
digits = datasets.load_digits()

# CAH
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
matrice_liens = linkage(digits.data, method='ward')

# K-means
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=10, random_state=42)
clusters = kmeans.fit_predict(digits.data)
```

## ğŸ“ˆ RÃ©sultats attendus

- **Dendrogramme** illustrant la structure hiÃ©rarchique des chiffres
- **Matrices de confusion** comparant les deux mÃ©thodes
- **Visualisations** des clusters formÃ©s
- **Analyse** des similitudes visuelles entre chiffres regroupÃ©s

## ğŸ” Points clÃ©s d'analyse

- Identification du nombre optimal de clusters via le dendrogramme
- Comparaison des performances CAH vs K-means
- Observation des concordances/divergences entre les deux approches
- InterprÃ©tation des regroupements basÃ©s sur les similaritÃ©s visuelles

## ğŸ“Š Guide d'interprÃ©tation des rÃ©sultats

### InterprÃ©tation du clustering CAH
Points Ã  analyser :
- **Groupements formÃ©s** : Quels chiffres sont regroupÃ©s ensemble ?
- **Similitudes visuelles** : Identifier les formes, courbes et caractÃ©ristiques communes
- **Confusions courantes** : Expliquer pourquoi certains chiffres sont confondus
  - Exemple : 1 et 7 (traits verticaux)
  - Exemple : 3 et 8 (courbes similaires)
  - Exemple : 4 et 9 (structures proches)
- **Structure hiÃ©rarchique** : Observer l'ordre de fusion dans le dendrogramme

### Comparaison CAH vs K-means
Points Ã  analyser dans les matrices de confusion :
- **Concordances** : OÃ¹ les deux mÃ©thodes attribuent les mÃªmes clusters
- **Divergences** : DiffÃ©rences significatives entre les deux approches
- **Performance par chiffre** : Quelle mÃ©thode performe mieux pour certains chiffres spÃ©cifiques
- **StabilitÃ©** : CohÃ©rence des regroupements entre les deux mÃ©thodes
- **Avantages/InconvÃ©nients** :
  - CAH : Structure hiÃ©rarchique claire, interprÃ©table
  - K-means : Plus rapide, sensible Ã  l'initialisation

## ğŸ“š Concepts thÃ©oriques

### CAH (Hierarchical Clustering)
- Approche **bottom-up** : chaque Ã©lÃ©ment commence comme cluster individuel
- Fusion progressive basÃ©e sur la **distance euclidienne**
- ReprÃ©sentation via **dendrogramme**
- MÃ©thode Ward pour minimiser la variance intra-cluster

### K-means
- Partitionnement en **K clusters prÃ©dÃ©finis**
- Initialisation de K centres (centroids)
- Assignation itÃ©rative + rÃ©ajustement des centres
- Convergence jusqu'Ã  stabilisation

## ğŸ“„ Livrables

- Code Python complet et commentÃ©
- Visualisations (dendrogrammes, matrices de confusion)
- InterprÃ©tations des rÃ©sultats
- Analyse comparative des deux mÃ©thodes

## ğŸ‘¨â€ğŸ“ Auteur

**RÃ©alisÃ© par :** Hsan Khecharem

**FiliÃ¨re :** Licence en Sciences de l'Informatique  
**SpÃ©cialitÃ© :** GÃ©nie Logiciel et SystÃ¨mes d'Information  
**FacultÃ© :** FacultÃ© des Sciences de Sfax

- Email : khecharemhsan@gmail.com

---

*Pour toute question concernant ce TP, n'hÃ©sitez pas Ã  consulter la documentation de scikit-learn et scipy.*
